---
title: "Forecasting Strength Levels of Raw Powerlifters"
author: "Matt Naples"
output: html_document
knit: (function(input_file, encoding) {
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), 'index.html'))})
---

```{r, include = F}
library(tidyverse)
library(plotly)
library(ggridges)
library(stringr)
library(lubridate)
library(scales)
library(leaps)
library(TSA)
library(tseries)
library(astsa)
#install.packages('tinytex')
#tinytex::install_tinytex()  # install TinyTeX
```

```{r,include = F}
d<- read_csv("openpowerlifting.csv")
d %>% group_by(Name) %>% summarize(n=n())
```



# Background
My dataset is the "openpowerliting.csv" dataset downloaded from [kaggle](https://www.kaggle.com/open-powerlifting/powerlifting-database/data) and generated by [openpowerlifting.org](https://www.openpowerlifting.org/). Each athlete in one powerlifting meet typically performs three attempts at the bench press, three attemps at the squat, and three attempts at the deadlift. This dataset essentially collects all lift attempts for most or all lifters in worldwide powerlifting meets since the 1960s. (I'll likely eliminate years <1980 because data is limited for that time period). There are different equipment divisions—equipment being what you are allowed to wear during competion. There are "Single-ply", "Multi-ply", "Raw", "Wraps" divisions. Single- and multi-ply indicate that the competitor can wear high-elasticity suit (for squat and deadlift) and shirt (for bench). This enables the competitor to lift more weight. "Wraps" is a class where the competitor can wear knee wraps (again a piece of equipment enabling one to lift more weight). The "Raw" class means that the user is allowed to wear only a belt, wrist wraps, and perhaps sanctioned knee sleeves—no suit or knee wraps allowed. In addition, the dataset contains the competitor's sex, and weight class. 

I'll be looking to see the performance of competitors over time. The performance metric of interest will be the `IPFPoints`.`IPFPoints` are a transformation of TotalKg, where TotalKg = max(all three bench attempts)+max(all 3 squat attempts)+ max(all 3 deadlift attempts). The transformation has been designed to normalize `TotalKg` across gender and weight class so that awards can be given for best "overall" lifter across men, women, and weight class. It is a measure of relative performance within one's weight and gender class. If the performance is very high with respect to the average and to the variance of a given weightclass,then the score is higher. The `Wilks` score is another similar measure, but I'm going to use `IPFPoints`, because they were developed more recently and with a larger set of data. The metric, however, is not adequate for comparison across different divisions in all instances. Therefore, I will have to consider a more nuanced approach.


# Tailoring the data

For my analysis, I'm going to restrict the data in the following ways:

1.) Get rid of years before 1980, as there is not nearly as much data collected before that time, which means that if I look at the average of each month or quarter, there may be insufficient counts...

```{r,echo=F}
ggplot(d %>% group_by(yearMonth=floor_date(Date,"month")) %>% summarize(numCompetitorsInAGivenMonth=n()) , aes(x=yearMonth,y=numCompetitorsInAGivenMonth))+geom_line()+geom_point(alpha=.5,aes(color=factor(ifelse(numCompetitorsInAGivenMonth>=50,"yes","no")))) + labs(color=">50 competitors in the given month?")

```

2.) Remove the year 2019 whenever I am looking at aggregated statistics for a whole year, since 2019 is not completed in the data. The max `Date` is `r max(d$Date)`.

3.) Remove all instances where the competitor did not perform all 3 lifts. In my opinion, having strength in the squat, bench press, and deadlift simultaneously is a different quality than having strength at any combination of the 3 (i.e just bench press, just deadlift, or bench press and squat, etc).





```{r}
#Data constraint

#d<- d %>% filter(year(Date)>1979,Event=="SBD",!(Federation %in% c("THSPA", "THSWPA")) ) 
d<- d %>% filter(year(Date)>1979,Event=="SBD") 
```

```{r,echo=F}
#averaging by Equipment, time period
tsdE<-d %>% group_by(yearMonth=floor_date(Date,"month"),Equipment) %>% summarize(avgIPFPoints=mean(IPFPoints,na.rm=T),medianIPFPts=median(IPFPoints,na.rm=T),avgWilks=mean(Wilks,na.rm=T),avgTotal=mean(TotalKg,na.rm=T),n=n()) %>% filter(year(yearMonth)>1979,n>25)
tsdEY<-d %>% group_by(year=year(floor_date(Date,"year")),Equipment) %>% summarize(avgIPFPoints=mean(Wilks,na.rm=T),avgWilks=mean(Wilks,na.rm=T),avgTotal=mean(TotalKg,na.rm=T),n=n()) %>% filter(year>1979,year<2019,n>100)




tsdESeason<- d %>% group_by(season=floor_date(Date,"season"),Equipment) %>% summarize(avgIPFPoints=mean(Wilks,na.rm=T),avgWilks=mean(Wilks,na.rm=T),avgTotal=mean(TotalKg,na.rm=T),n=n()) %>% filter(year(season)>1979,n>100)

tsdEQuarter<- d %>% group_by(quarter=floor_date(Date,"quarter"),Equipment) %>% summarize(avgIPFPoints=mean(Wilks,na.rm=T),avgWilks=mean(Wilks,na.rm=T),avgTotal=mean(TotalKg,na.rm=T),n=n()) %>% filter(year(quarter)>1979,n>100)

tsdESemester<- d %>% group_by(semester=floor_date(Date,"halfyear"),Equipment) %>% summarize(avgIPFPoints=mean(Wilks,na.rm=T),avgWilks=mean(Wilks,na.rm=T),avgTotal=mean(TotalKg,na.rm=T),n=n()) %>% filter(year(semester)>1979,n>100)
 
#averaging by Sex, month
tsdS<- d %>% group_by(yearMonth=floor_date(Date,"month"),Sex) %>% summarize(avgIPFPoints=mean(IPFPoints,na.rm=T),avgTotal=mean(TotalKg,na.rm=T),avgWilks=mean(Wilks,na.rm=T),n=n()) %>% filter(year(yearMonth)>1979,n>25)
```

# Questions of interest: How have strength measures for Raw powerlifting been changing since the start of its growth spurt?

Powerlifting has gained immense popularity since around 2012. See the following chart (n=number of non-unique competitors, one person is counted each time they compete):

```{r,echo=F}
ggplot( d %>%  group_by(year=floor_date(Date,"year")) %>% summarize(n=n()) %>% filter(year(year)<2019),aes(y=n,x=year))+geom_line()
```

More specifically, much of this growth is primarily due to new competitors in the Raw division...

```{r,echo=F}
 ggplot( d %>% filter(year(Date)<2019) %>%  group_by(Equipment,year=floor_date(Date,"year")) %>% summarize(n=n()),aes(y=n,x=year))+geom_line(aes(color=Equipment))
#ggplot( tsdES,aes(y=n,x=yearMonth))+geom_line(aes(color=interaction(Sex,Equipment)))
#plotly::ggplotly( ggplot(d %>% filter(Federation %in% BiggestFeds$Federation) %>% group_by(Federation,Equipment,year=floor_date(Date,"year")) %>% summarize(n=n()),aes(year,n,color=interaction(Equipment,Federation)))+geom_line() )


#d %>% filter(Federation %in% BiggestFeds$Federation) %>% group_by(Federation,Equipment,year=floor_date(Date,"year")) %>% filter(year(year)>2012) %>%  group_by(Federation,Equipment) %>% summarize(n=n()) %>% arrange(desc(n))
```

Since I'm personally interested in the Raw powerlifting division, I'd like to see how the strengh of Raw powerlifters has been changing since more individual's began competing around 2012. To do this, I will summarize the constrained data by finding the average IPFPoints among Raw competitors for each month.
```{r,echo=F}
head(tsdE)
```

Below is the Raw division's average IPFPoints for each month in the constrained dataset. Red points indicate lower sample size for that month. During and after 2012, each month's sample size seems to be large enough to estimate the monthly average reasonably well. I opt to average the IPFPoints by month (as opposed to quarter,year,etc.) since there are plenty of competitors within each month over which I can average, and that will give a sample size of 88 monthly averages. Also, grouping by month isn't so granular and noisy that it drowns out the linear trend.

```{r,echo=F}

ggplot ((tsdE %>% filter(Equipment=="Raw")) ,aes(yearMonth,avgIPFPoints))+geom_line() + geom_point(aes(color=n)) + scale_color_gradientn(colours = c("red","blue","green"),limits=c(25,1000) )

#mn<- mean((d %>% filter(Equipment=="Raw") %>% dplyr::select(IPFPoints))$IPFPoints,na.rm=T)
#sdv<- sd((d %>% filter(Equipment=="Raw") %>% dplyr::select(IPFPoints))$IPFPoints,na.rm=T)
#pnorm(800,mn,sdv,lower.tail=F) * nrow(d %>% filter(Equipment=="Raw", !is.na(IPFPoints)))
#d %>% filter(IPFPoints>800) %>% nrow()
```

I perform one final constraint to eliminate years before 2012. This is the final dataframe that I will use to model and forecast avgIPFPoints...

```{r,echo=F}
tsdRaw<- tsdE %>% filter(year(yearMonth)>=2012,Equipment=="Raw")
#tsdRawQ<- tsdEQuarter %>% filter(year(quarter)>=2013, quarter<date("2019-04-01"),Equipment=="Raw")
dataSet<- c(rep("train",nrow(tsdRaw)-5),rep("test",nrow(tsdRaw) - (nrow(tsdRaw)-5)))
tsdRaw<- cbind.data.frame(tsdRaw,dataSet)
colrs<- c("#F592E1","#30C3E1","#98B2F9","#3CC8AC","#B9B761","#82C379","#DFA96A")
```

```{r,echo=F}
head(tsdRaw)
```
More properties about the dataset...

* The sample size is `r nrow(tsdRaw)`.
* ranging from `r min(tsdRaw$yearMonth)` to `r max(tsdRaw$yearMonth)`.
* the mean average monthly IPFPoints across the whole dataset is `r mean(tsdRaw$avgIPFPoints,na.rm=T)`.
* the standard deviation in average monthly IPFPoints across the whole dataset is `r sd(tsdRaw$yearMonth, na.rm=T)`.

Below is the time series plot of the *training* data. I've omitted the most recent 5 observations to so I can see how well the model predicts the last 5 observations. 



```{r,echo=F}
 ggplot ((tsdRaw %>% filter(dataSet=="train")) ,aes(yearMonth,avgIPFPoints))+geom_line() + geom_point(aes(color=n)) + scale_color_gradientn(colours = c("red","green","blue"),limits=c(50,1000) )  
```




```{r,echo=F}
bc<-BoxCox.ar(tsdRaw$avgIPFPoints)
```


I opt not to transform this data, as I would lose a lot of interpretation by using a boxcox transformation where the mle for lambda = `r bc$mle `



```{r,echo=F}
adf.test(tsdRaw$avgIPFPoints)

```

An adf test suggests that the data is a deterministic linear trend + a starionary process. This is plausible after looking at the time series plot above as well.

# Detrending the data: three linear deterministic trends

```{r,include=F}
tsdRaw <- tsdRaw %>% mutate(quarter= (quarter(yearMonth)),month=month(yearMonth,label=T),biMonth=month(floor_date(yearMonth,"bimonth"),label = T))
tsdRaw$quarter<- as.factor(tsdRaw$quarter)
tsdRaw$month<- factor(tsdRaw$month,ordered = F)
tsdRaw$biMonth<- factor(tsdRaw$biMonth,ordered=F)
m<- lm(data=tsdRaw %>% filter(dataSet=="train"), formula = avgIPFPoints~yearMonth+month) 
```


```{r,include=F}
#(summary(subs)$bic)[which.min((summary(subs))$bic)]
#summary(subs)$bic
tsdRaw <- tsdRaw%>% mutate(monthFeb=ifelse(month=="Feb",1,0), monthMay =ifelse(month=="May",1,0),monthJun=ifelse(month=="Jun",1,0))
train <- tsdRaw %>% filter(dataSet=="train")
lTrend<- lm(train,formula = avgIPFPoints~yearMonth+monthFeb+monthMay+monthJun) 
#predicted<- 285.98715358 + 0.01275973*(train$yearMonth %>% as.numeric) -14.36783452*train$monthFeb -12.18520369*train$monthMay + 18.69464064*train$monthJun
#r<- train$avgIPFPoints-predicted
r<-lTrend$residuals
lTrend2<- lm(train, formula = avgIPFPoints~yearMonth + quarter)
lTrend3<- lm(train,formula= avgIPFPoints~yearMonth+month)
```


I will fit three different linear trends. All three will of course contain a slope estimate for time t (yearMonth), but they will be different in the following ways:

* the first detrending method (used for final models 1-3 later on) will contain the mean for February, May, June (with mean of all other months as baseline)...

```{r,echo=F}
summary(lTrend)
```

* the second detrending method (used for final models 4-6 later on) will contain the seasonal means for each of the 4 quarters of the year...

```{r,echo=F}
summary(lTrend2)
```

* the third detrending method (used for final model 7 later on) will contain the seasonal means for each month of the year...

```{r,echo=F}
summary(lTrend3)
```










# A brief overview of how I will compare models

Okay, at this point, there will be many moving parts and comparisons. It is a good idea to provide a brief overview of how I plan to compare models. 

*First*: Since models 1 through 3 use the same detrended data (`detrended`), I will find a pool of 3 models using acf, pacf, and armasubsets. Then I will then compare the AIC, AICc, and BIC of these three models and select the first "candidate" model that will be compared with the second and third candidate models mentioned below.

*Second*: Since modes 4 through 6 use the same detrended data (`detrended2`), I will find a pool of 3 models using acf, pacf, and arma subsets. Then I will then compare the AIC, AICc, and BIC of these three models and select the second "candidate" model.

*Third*: Since models 7 and 8 below use the same detrended data (`detrended3`), I will find a pool of 2 models using acf, pacf, and arma subsets. Then I will then compare the AIC, AICc, and BIC of these two models and select the third "candidate" model. I only use 2 models here because one of the top 3 models threw an error when I attempted to call sarima() ("non-finite finite-difference value"). I couldn't manage to remedy the error.

*Fourth*: Once the three candidate models are selected, I will compare their residuals and SSE's (based on 5 observations reserved for testing). I will select the two best models from these three candidates.

*Fifth*: Once the two candidate models are selected, I will refit those models using the entire dataset and perform a future forecast for both models.

#Finding the three candidate ARMA(p,q) processes. Each will model one of the 3 detrended datasets.

## Finding the 1st candidate model

### selecting three tentative ARMA(p,q) processes
Let's start by detrending the data, plotting the detrended data, its acf, pacf, and armasubsets...
```{r,echo=F}
# detrend  data
detrended<-train %>% dplyr::select(yearMonth,avgIPFPoints) %>% bind_cols(data.frame(detrend=r))
plot(ts(detrended$detrend),type="o")
#ggplot(resids,aes(avgIPFPoints,residuals))+geom_point()
#lm(data=resids,formula=residuals~avgIPFPoints) %>% summary()
acf(detrended$detrend,ci.type="ma",lag.max=25)
pacf(detrended$detrend,lag.max = 25)
subsets<- armasubsets(detrended$detrend,14,14)
plot(subsets)
```


The top 3 models for the `detrended` data by BIC are...

* 1.) model 1: AR(12), where the 1st and 12th AR coefficients are the only nonzero coefficients.
* 2.) model 2: ARMA(12,12) where the 12th AR coefficient and the 1st and 12th MA coefficients are the only nonzero coefficients.
* 3.) model 3: AR(12) where only the 12th AR coefficient is the only nonzero.

```{r,include=F}

## significant reg coeff models

# model 1: AR(12), where the 1st and 12th AR coefficients are the only nonzero coefficients.
m1<-sarima(detrended$detrend,12,0,0,fixed=c(NA,rep(0,10),NA,0))

#m1NoConstant<- sarima(detrended$detrend,12,0,0,fixed=c(NA,rep(0,10),NA,0))

#model 2: ARMA(12,12) where the 12th AR coefficient and the 1st and 12th MA coefficients are the only nonzero coefficients.
m2<- sarima(detrended$detrend,12,0,12,fixed=c(rep(0,11),NA,NA,rep(0,10),NA,0))

#model 3: AR(12) where only the 12th AR coefficient is the only nonzero.
m3<- sarima(detrended$detrend,12,0,0,fixed=c(rep(0,11),NA,0))

```

Here is the first model fit
```{r,echo=F}
m1$ttable
```

and the second model fit
```{r,echo=F}
m1$ttable
```


and the third model fit
```{r,echo=F}
m1$ttable
```

### comparing AIC,AICc,BIC for models 1-3

After fitting these models, here are the AIC,AICc,and BIC comparisons:



```{r, echo = F}
tmp<- data.frame(model=c("m1","m2","m3"),AIC = c(m1$AIC,m2$AIC,m3$AIC), AICc = c(m1$AICc,m2$AICc,m3$AICc),BIC=c(m1$BIC,m2$BIC,m3$BIC))
tmpLong<- tmp %>% gather(key="metric",value="value",c(AIC,AICc,BIC))

ggplot(tmpLong,aes(y=value,x=model,fill=metric)) + geom_bar(stat="identity",position="dodge")+ geom_text(aes(label=round(value,4)), position=position_dodge(width=0.9), vjust=-0.25)+facet_wrap(~metric)
                                                               
```

As you can see, model 1 (an AR(12) model where only the 1st and 12th AR coefficient are estimated as nonzero) performs the best across all 3 metrics on `detrended`. In addition, it is the simplest model with very nice interpretation (Y_t is a function of the previous year's observation during the same month). So it will be chosen as the candidate model.


## Finding the 2nd candidate model

### selecting three tentative ARMA(p,q) processes

```{r,echo=F}
detrended2<-train %>% dplyr::select(yearMonth,avgIPFPoints) %>% bind_cols(data.frame(detrend=lTrend2$residuals))
plot(ts(detrended2$detrend),type="o")
#ggplot(resids,aes(avgIPFPoints,residuals))+geom_point()
#lm(data=resids,formula=residuals~avgIPFPoints) %>% summary()
acf(detrended2$detrend,ci.type="ma",lag.max=25)
pacf(detrended2$detrend,lag.max = 25)
subsets2<- armasubsets(detrended2$detrend,14,14)
plot(subsets2)
```

The top 3 models for `detrended2`  in order by BIC are...

* 1.) model 4: AR(12), where only the 4th and 12th AR coefficients are nonzero. This model had an insignificant 4th AR coefficient, and removing it would make it identical to model 5, so we keep it for now only for comparison purposes.
* 2.) model 5: AR(12), where only the 12th AR coefficient is nonzero.
* 3.) model 6: ARMA(12,3) where only the ar4, ar12, and ma3 coefficients are nonzero. This third model had a nonsignificant 4th AR coefficient as well, and removing it does lead to a lower AIC,AICc,and BIC. So, model 6 will still be ARMA(12,3), but only ar12 and ma3 coefficients are nonzero.

```{r,include=F}
### all quarters
 m4<- sarima(detrended2$detrend,12,0,0,fixed=c(0,0,0,NA,0,0,0,0,0,0,0,NA,0))
 m5<- sarima(detrended2$detrend,12,0,0,fixed=c(rep(0,11),NA,0))
 m5$ttable
 #ar(12), ar12
 m6<- sarima(detrended2$detrend,12,0,3,fixed=c(0,0,0,NA,0,0,0,0,0,0,0,NA,0,0,NA,0))
m6$ttable

m6Noar4<-sarima(detrended2$detrend,12,0,3,fixed=c(0,0,0,0,0,0,0,0,0,0,0,NA,0,0,NA,0))

#m6 without ar4 will be model 6 choice...
m6<-sarima(detrended2$detrend,12,0,3,fixed=c(0,0,0,0,0,0,0,0,0,0,0,NA,0,0,NA,0))
#m4wins
```


Here is the fourth model fit
```{r,echo=F}
m4$ttable
```

and the fifth model fit
```{r,echo=F}
m5$ttable
```


and the 6th model fit
```{r,echo=F}
m6$ttable
```

### comparing AIC,AICc,BIC for models 4-6



```{r, echo = F}
tmp<- data.frame(model=c("m4","m5","m6"),AIC = c(m4$AIC,m5$AIC,m6$AIC), AICc = c(m4$AICc,m5$AICc,m6$AICc),BIC=c(m4$BIC,m5$BIC,m6Noar4$BIC))
tmpLong<- tmp %>% gather(key="metric",value="value",c(AIC,AICc,BIC))

ggplot(tmpLong,aes(y=value,x=model,fill=metric)) + geom_bar(stat="identity",position="dodge")+ geom_text(aes(label=round(value,4)), position=position_dodge(width=0.9), vjust=-0.25)+facet_wrap(~metric)
                                                               
```

Model 6 is the best model in terms of AIC, and AICc. However, model 5 is the best in terms of BIC. In addition, it is the simplest and most interpretable model with only one significant coefficient (ar12). For these reasons, model 5 is the second candidate model.


## Finding the 3rd candidate model

### selecting two tentative ARMA(p,q) processes


```{r,echo=F}

detrended3<-train %>% dplyr::select(yearMonth,avgIPFPoints) %>% bind_cols(data.frame(detrend=lTrend3$residuals))
plot(ts(detrended3$detrend),type="o")
#ggplot(resids,aes(avgIPFPoints,residuals))+geom_point()
#lm(data=resids,formula=residuals~avgIPFPoints) %>% summary()
acf(detrended3$detrend,ci.type="ma",lag.max=25)
pacf(detrended3$detrend,lag.max = 25)
subsets3<- armasubsets(detrended3$detrend,14,14)
plot(subsets3)

```

The top 2 models for the `detrended3` data by BIC are...

* 1.) model 7: ARMA(12,1), where only the ar12 and ma1 coefficients are nonzero.
* 2.) model 8: ARMA(14,8), where only ar1,ar8,ar12,ar14,and ma8 coefficients are nonzero.

```{r,include=F}
### all months

#arma(12,1) ar12, ma1
 m7<- sarima(detrended3$detrend,12,0,1,fixed=c(rep(0,11),NA,NA,0))
 m7$ttable
 
#arma(14,8) ar 1, 8, 12, 14... ma 1, 8
 m8<- sarima(detrended3$detrend,14,0,8,fixed=c(NA,0,0,0,0,0,0,NA,0,0,0,NA,0,0,NA,0,0,0,0,0,0,NA,0))
 m8$ttable
```

Here is the 7th model fit
```{r,echo=F}
m7$ttable
```

and the 7th model fit
```{r,echo=F}
m8$ttable
```

### Comparing AIC, AICc, and BIC for models 7 and 8
```{r, echo = F}
tmp<- data.frame(model=c("m7","m8"),AIC = c(m7$AIC,m8$AIC), AICc = c(m7$AICc,m8$AICc),BIC=c(m7$BIC,m8$BIC))
tmpLong<- tmp %>% gather(key="metric",value="value",c(AIC,AICc,BIC))

ggplot(tmpLong,aes(y=value,x=model,fill=metric)) + geom_bar(stat="identity",position="dodge")+ geom_text(aes(label=round(value,4)), position=position_dodge(width=0.9), vjust=-0.25)+facet_wrap(~metric)
```
Model 7 outperforms model 8 and will thus be the third candidate model. 


# Comparing candidate models

Okay, so models 1, 5, and 7 are the candidate models. Let's compare them using residuals and SSE.

## Comparing residuals of candidate models

```{r}
m1<-sarima(detrended$detrend,12,0,0,fixed=c(NA,rep(0,10),NA,0))
m5<- sarima(detrended2$detrend,12,0,0,fixed=c(rep(0,11),NA,0))
m7<- sarima(detrended3$detrend,12,0,1,fixed=c(rep(0,11),NA,NA,0))
```

The residuals of model 5 seem to have some autocorrelation in the residuals, where models 1 and 7 do not. However, model 5 appears to be closer to normal (it is the only nonsignificant p-value in the shapiro-wilks test.

```{r,echo=F}
shapiro.test(m1$fit$residuals)
shapiro.test(m5$fit$residuals)
shapiro.test(m7$fit$residuals)

```

## Comparing candidate model predictions against true values using SSE

```{r,include=F}

## significant reg coeff models

m1.forecast<-sarima.for(r,n.ahead=5,12,0,0,fixed=c(NA,rep(0,10),NA,0))
# ar(12) only 12th coeff no constant
m2.forecast<-sarima.for(r,n.ahead=5,12,0,12,fixed=c(rep(0,11),NA,NA,rep(0,10),NA,0))
#arma(12,12) 1st and 12th AR coeff, 1st and 12th ma coeffs
m3.forecast<-sarima.for(r,n.ahead=5,12,0,0,fixed=c(rep(0,11),NA,0))
test<- tsdRaw %>% filter(dataSet=="test")


m4.forecast<- sarima.for(detrended2$detrend,n.ahead=5,12,0,0,fixed=c(0,0,0,NA,0,0,0,0,0,0,0,NA,0))
m5.forecast<- sarima.for(detrended2$detrend,n.ahead=5,12,0,0,fixed=c(rep(0,11),NA,0))
m6.forecast<- sarima.for(detrended2$detrend,n.ahead=5,12,0,3,fixed=c(0,0,0,NA,0,0,0,0,0,0,0,NA,0,0,NA,0))
m7.forecast<- sarima.for(detrended3$detrend,n.ahead=5,12,0,1,fixed=c(rep(0,11),NA,NA,0))
#arma(12,1) ar12, ma1

 #arma(14,8) ar1,ar8,ar12,ar14,ma8
arimaPart<- data.frame(m1Pred=m1.forecast$pred,m2Pred=m2.forecast$pred,m3Pred=m3.forecast$pred,m4Pred=m4.forecast$pred,m5Pred=m5.forecast$pred,m6Pred=m6.forecast$pred,m7Pred=m7.forecast$pred)
linearPart1<- predict.lm(lTrend,tsdRaw %>% filter(dataSet=="test"))
linearPart2<- predict.lm(lTrend2,tsdRaw %>% filter(dataSet=="test"))
linearPart3<- predict.lm(lTrend3,tsdRaw %>% filter(dataSet=="test"))

forecasted<- tsdRaw %>% filter(dataSet=="test") %>% dplyr::select(yearMonth) %>% data.frame(.,m1Pred=arimaPart$m1Pred+linearPart1, m5Pred=arimaPart$m5Pred+linearPart2 ,m7Pred=arimaPart$m7Pred+linearPart3)

#forecasted<- tsdRaw %>% filter(dataSet=="test") %>% dplyr::select(yearMonth) %>% data.frame(.,m1Pred=arimaPart$m1Pred+linearPart, m2Pred=arimaPart$m2Pred+linearPart ,m3Pred=arimaPart$m3Pred+linearPart,m4Pred=arimaPart$m4Pred+linearPart,m5Pred=arimaPart$m5Pred+linearPart,m6Pred=arimaPart$m6Pred+linearPart,m7Pred=arimaPart$m7Pred+linearPart)

foreLong<- forecasted %>% gather(key ="model",value = "prediction", c(m1Pred,m5Pred,m7Pred))
```


Here is the predictions for Model 1:

```{r,echo=F}
m1.forecast<-sarima.for(r,n.ahead=5,12,0,0,fixed=c(NA,rep(0,10),NA,0))
test<- tsdRaw %>% filter(dataSet=="test")
```

for Model 5:

```{r,echo=F}
m5.forecast<- sarima.for(detrended2$detrend,n.ahead=5,12,0,0,fixed=c(rep(0,11),NA,0))
```

and for Model 7:

```{r,echo=F}
m7.forecast<- sarima.for(detrended3$detrend,n.ahead=5,12,0,1,fixed=c(rep(0,11),NA,NA,0))
```

Here is what the forecast looks like for the three models when looking at the actual data (as opposed to the detrended data):

```{r,echo=F}
ggplot(tsdRaw,aes(y=avgIPFPoints,x=yearMonth))+geom_line()+geom_point()+geom_line(data=foreLong,aes(x=yearMonth,y=prediction,color=model)) +geom_point(data=foreLong,aes(x=yearMonth,y=prediction,color=model))
```

```{r,include=F}
#model 1 SSE
sum(( test$avgIPFPoints  -(linearPart1+arimaPart$m1Pred) )^2)

#model 2 SSE
sum(( test$avgIPFPoints  -(linearPart1+arimaPart$m2Pred) )^2)

#model 3 SSE #arma(12,12) 1st and 12th AR coeff, 1st and 12th ma coeffs

sum(( test$avgIPFPoints  -(linearPart1+arimaPart$m3Pred) )^2)

#model 4 SSE
sum(( test$avgIPFPoints  -(linearPart2+arimaPart$m4Pred) )^2)

#model 5 SSE  #arma(12,12) ar12,e1,e12 OLD VERSION with 146 sse
sum(( test$avgIPFPoints  -(linearPart2+arimaPart$m5Pred) )^2)

#model 6 SSE
sum(( test$avgIPFPoints  -(linearPart2+arimaPart$m6Pred) )^2)

#model 7 SSE
sum(( test$avgIPFPoints  -(linearPart3+arimaPart$m7Pred) )^2)
```

Let's compare the SSE between the three candidate models.

```{r}
#model 1 SSE
sum(( test$avgIPFPoints  -(linearPart1+arimaPart$m1Pred) )^2)

#model 5 SSE
sum(( test$avgIPFPoints  -(linearPart2+arimaPart$m5Pred) )^2)

#model 7 SSE
sum(( test$avgIPFPoints  -(linearPart3+arimaPart$m7Pred) )^2)
```

Since model 5 had autocorrelation in the residuals and by far the highest SSE, the top two models for forecasting will be models 1 and 7.


# Refitting top two models with the whole dataset for forecast
Now I will fit models 1 and 7 to the whole dataset

```{r,include=F}
# detrend the data
lTrendFull<- lm(tsdRaw,formula = avgIPFPoints~yearMonth+monthFeb+monthMay+monthJun) 
detrendedf<-tsdRaw %>% dplyr::select(yearMonth,avgIPFPoints) %>% bind_cols(data.frame(detrend=lTrendFull$residuals))
m1f<- sarima(detrendedf$detrend,12,0,0,fixed=c(rep(0,11),NA,0))


lTrend3Full<- lm(tsdRaw,formula= avgIPFPoints~yearMonth+month)
detrendedf3<-tsdRaw %>% dplyr::select(yearMonth,avgIPFPoints) %>% bind_cols(data.frame(detrend=lTrend3Full$residuals))
m7f<- sarima(detrended3$detrend,12,0,1,fixed=c(rep(0,11),NA,NA,0))

```
## top 2 models' coefficients
These are the new coefficients used to detrend the data when considering the whole dataset for model 1:
```{r, echo = F}
lTrendFull %>% summary()
```

and the new coefficients used to detrend the data when considering the whole dataset for model 7:

```{r, echo= F}
lTrend3Full %>% summary()
```



Here is the fit for the first model ARIMA model when including all data points (not just training data):

```{r,echo=F}
m1f$ttable
m1f$fit
```

and here is the fit for the 7th  ARIMA model when including all data points (not just training data):

```{r,echo=F}
m7f$ttable
m7f$fit
```


## top 2 model's equation


These are the following model Equations:

Let t = the yearMonth, Y_t = the avgIPFPoints at t...

### Model 1


$$
Y_t= 292.91376 + 0.01233t - 14.16329(monthFeb) - 11.95537(monthMay) + 18.93788(monthJun) + 0.3485Y_{t-12} +e_t
$$

$$
e_t \sim WN(0, \sigma^2 \approx 141.5 )
$$



### Model 7

$$
Y_t =  286.08882 +  0.01229t -6.68611(monthFeb) +  6.39329(monthMar) +  9.50232(monthApr)  -4.48185(monthMay)  +26.41261(monthJun) +  15.46817(monthJul) +  6.03766(monthAug)+ 3.29731(monthSep)   +13.14799(monthOct) +  6.26134(monthNov)  + 8.13320(monthDec) + 0.2083045 Y_{t-12}  + e_t -0.2637751 e_{t-1}
$$

$$
e_t \sim WN(0, \sigma^2 \approx 133.6 )
$$


# Top Models' Forecasts
Now, let's use these models to forecast one year into the future!


```{r,echo=F}
nAhead<- 12

m1f.forecast<- sarima.for(detrendedf$detrend,n.ahead=nAhead,12,0,0,fixed=c(rep(0,11),NA,0))
m7f.forecast<- sarima.for(detrendedf3$detrend,n.ahead=nAhead,12,0,1,fixed=c(rep(0,11),NA,NA,0))
forecastVals<- data.frame(yearMonth = max(tsdRaw$yearMonth) %m+% months(1:nAhead))
forecastVals<- forecastVals %>% mutate(quarter=as.factor(quarter(yearMonth)),month=as.factor(month(yearMonth,label=T)),monthFeb=ifelse(month=="Feb",1,0), monthMay =ifelse(month=="May",1,0),monthJun=ifelse(month=="Jun",1,0))
linearPartf1<- predict.lm(lTrendFull,forecastVals)


forecastVals<- data.frame(yearMonth = max(tsdRaw$yearMonth) %m+% months(1:nAhead))
forecastVals<- forecastVals %>% mutate(month=as.factor(month(yearMonth,label=T)))
linearPartf3<- predict.lm(lTrend3Full,forecastVals)


arimaPart<- data.frame(m1Pred=m1f.forecast$pred,m7Pred=m7f.forecast$pred)
arimaPart
arimaPart$m7Pred+linearPartf3

futurePreds<- data.frame(yearMonth=forecastVals$yearMonth,m1AllData =arimaPart$m1Pred +linearPartf1, m7AllData = arimaPart$m7Pred+linearPartf3 )
#forecasted<- tsdRaw %>% filter(dataSet=="test") %>% dplyr::select(yearMonth) %>% data.frame(.,m1Pred=arimaPart$m1Pred+linearPart, m2Pred=arimaPart$m2Pred+linearPart ,m3Pred=arimaPart$m3Pred+linearPart,m4Pred=arimaPart$m4Pred+linearPart,m5Pred=arimaPart$m5Pred+linearPart,m6Pred=arimaPart$m6Pred+linearPart,m7Pred=arimaPart$m7Pred+linearPart)

futurePredsLong<- futurePreds %>% gather(key="model",value="prediction",c(m1AllData, m7AllData))
ggplot(tsdRaw,aes(y=avgIPFPoints,x=yearMonth))+geom_line()+geom_point()+geom_line(data=futurePredsLong,aes(x=yearMonth,y=prediction,color=model))+geom_point(data=futurePredsLong, aes(x=yearMonth, y= prediction,color=model) )
```


# Conclusion

Models 1 and 7, the two final models, were very similar in that the observation from previous year at the same month (Y_{t-12}) was significant and positive, so the higher the average strength was one year prior to month t, the higher we expect this year at month t to be, holding other variables constant. It would also look that competing in the winter negatively affects strength scores and competing in summer months positively effects strength scores in general. It would be interesting to research the topic further, but my first guess is that this is due to competitive meets being placed in the summer and less competitive meets being in the winter on average. In addition, I do not really think this model can go on linearly forever. The population of lifters may get stronger over time, but that can really only happen up to a point. I do think there's a lot of room for improvement in the population as it continues to grow and training methodology keeps improving for the foreseeable future, however. I will be interested to see how my model performs over the next few years as I watch these numbers change!  





